# Crawler

### Setup:

- Make sure Node is installed. Tested with v20.12.1, v22.2.0
- Make sure npm is installed. Tested with 10.5.0, 10.8.1

```
# Install dependencies.
npm install

# Install default playwright browsers (chromium, firefox, webkit)
npx playwright install

# Perhaps install msedge or chrome if needed. (installed at root, could overwrite main browser)
npx playwright install chrome
npx playwright install msedge

# Get a list of command line options:
node ./src/main.mjs --help

# Run a crawl with custom proxy list.
node ./src/main.mjs -p list,of,proxy_urls -w radwell

# Run a crawl with automatic proxies generated by Apify. (need APIFY credentials)
APIFY_TOKEN=XYZ node ./src/main.mjs -p apify -w radwell

# Run a crawl without a proxy (might get blocked?)
node ./src/main.mjs -p skip -w radwell
```

---

### Assignment:

At Limble, we want to test your ability to scrape and process data using a scraping framework with a Chrome-based playwright crawler. Your task is to scrape product information from a vendor website and output it to a normalized format. The data scraping should happen within 30 seconds on demand, and the returned data needs to be grouped by attributes of the products that have been returned.

#### Project Requirements:

1. Scraping Task:
   - Use a javascript library utilizing a chrome-based playwright crawler to scrape product information from one of the [sites](#sites) below. (see [sites](#sites) section below)
   - Ensure the scraping process completes within 30 seconds.
1. Data Output:
   - Output the scraped data in a universal format such as JSON or CSV.
   - The data should include, at minimum if available, product name, price, availability, category, and any technical attributes.
1. [Bonus] Data Grouping:
   - Group the scraped data by product attributes. These attributes may be images, or structured data returned on the web page such as technical details.
   - Ensure the grouped data is easily readable and correctly categorized.
   - Utilize some form of AI technology as part of the grouping process.

- Considerations:
  - Ethical: Ensure the scraping process politely views the vendor's website to return data.
  - Performance: The scraping and data processing should be efficient and complete within the specified time frame.
  - Error Handling: Implement robust error handling to manage any issues that arise during the scraping process.
  - Scalability: Consider how the solution can be scaled if the volume of data increases.
  - Repository: Please create a zip file for this project and share it with us when you're done.

#### Sites

Here is the list of vendors that you can choose from:

- Radwell.com
- Bearingengineering.com
- 1000bulbs.com
- Lowes.com
- Acehardware.com
- Ferguson.com
- Uline.com

#### Implementation Notes

- Asking to complete in 30 seconds while making sure we scrape Ethically is a bit of a conflict. I've assumed that the 30 seconds was a hard limit, and to scrape as much as possible during those 30 seconds while also not overloading their servers with too many concurrent requests. After the 30 seconds, we stop and keep track of where we were in the process to resume later.
- I currently store the state in a simple sqlite database, but this could be converted to something more robust like Postgres pretty easily.
- Even though just 1 site was requested, I tried to make the configuration flexible enough so that other sites could be added later.
