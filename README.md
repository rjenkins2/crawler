# Crawler

### Setup:

- Make sure Node is installed. Tested with v20.12.1, v22.2.0
- Make sure npm is installed. Tested with 10.5.0, 10.8.1

```
# Install dependencies.
npm install

# Install default playwright browsers (chromium, firefox, webkit)
npx playwright install

# Perhaps install msedge or chrome if needed. (installed at root, could overwrite main browser)
npx playwright install chrome
npx playwright install msedge

# Get a list of command line options:
node ./src/main.mjs --help

# Run a crawl with custom proxy list.
node ./src/main.mjs -p list,of,proxy_urls -w radwell

# Run a crawl with automatic proxies generated by Apify. (need APIFY credentials)
APIFY_TOKEN=XYZ node ./src/main.mjs -p apify -w radwell

# Run a crawl without a proxy (might get blocked?)
node ./src/main.mjs -p skip -w radwell
```

---

### Assignment:

At Limble, we want to test your ability to scrape and process data using a scraping framework with a Chrome-based playwright crawler. Your task is to scrape product information from a vendor website and output it to a normalized format. The data scraping should happen within 30 seconds on demand, and the returned data needs to be grouped by attributes of the products that have been returned.

#### Project Requirements:

1. Scraping Task:
   - Use a javascript library utilizing a chrome-based playwright crawler to scrape product information from one of the [sites](#sites) below. (see [sites](#sites) section below)
   - Ensure the scraping process completes within 30 seconds.
1. Data Output:
   - Output the scraped data in a universal format such as JSON or CSV.
   - The data should include, at minimum if available, product name, price, availability, category, and any technical attributes.
1. [Bonus] Data Grouping:
   - Group the scraped data by product attributes. These attributes may be images, or structured data returned on the web page such as technical details.
   - Ensure the grouped data is easily readable and correctly categorized.
   - Utilize some form of AI technology as part of the grouping process.

- Considerations:
  - Ethical: Ensure the scraping process politely views the vendor's website to return data.
  - Performance: The scraping and data processing should be efficient and complete within the specified time frame.
  - Error Handling: Implement robust error handling to manage any issues that arise during the scraping process.
  - Scalability: Consider how the solution can be scaled if the volume of data increases.
  - Repository: Please create a zip file for this project and share it with us when you're done.

#### Sites

Here is the list of vendors that you can choose from:

- Radwell.com
- Bearingengineering.com
- 1000bulbs.com
- Lowes.com
- Acehardware.com
- Ferguson.com
- Uline.com

#### Implementation Notes

- Asking to complete in 30 seconds while making sure we scrape Ethically is a bit of a conflict. I've assumed that the 30 seconds was a hard limit, and to scrape as much as possible during those 30 seconds while also not overloading their servers with too many concurrent requests.
  - The goal is that after the 30 seconds, we save the state of the crawler and resume. I wasn't able to quite finish that, so that would be a future improvement. Crawlee saves the state automatically in storage/, so this shouldn't be too hard to implement.
- Even though just 1 site was requested, I tried to make the configuration flexible enough so that other sites could be added later. The goal was to make the crawler pretty generic and supply information in the config file for the website. See the configs directory, specifically the configs/radwell.json file for an example.
  - Currently enqueue, extract, exactOnly are the actions that can be taken on a particular page, with CSS selectors to determine what we care about. This could pretty easily be expanded to other actions, as needed.
  - Future plans include a way to try to auto-generate a configuration file for a website, perhaps. But it isn't too hard to generate one manually.
- Rudimentary tests have been implemented, but some mocking style tests of the dynamic router functionality would also probably be nice.
